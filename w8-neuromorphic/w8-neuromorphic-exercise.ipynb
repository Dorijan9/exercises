{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 - Neuromorphic computing - Exercise\n",
    "\n",
    "Note: this is a new version of the exercise, for the old version see [w8-neuromorphic-exercise-v1.ipynb](w8-neuromorphic-exercise-v1.ipynb).\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neuro4ml/exercises/blob/main/w8-neuromorphic/w8-neuromorphic-exercise.ipynb)\n",
    "\n",
    "## üß† Introduction \n",
    "\n",
    "Neuromorphic engineering is a field that aims to design and build artificial neural systems that mimic the architecture and principles of biological neural networks. Unlike traditional von Neumann computing architectures, neuromorphic chips:\n",
    "\n",
    "1. üîÑ Process information in a parallel, event-driven manner\n",
    "2. üíæ Integrate memory and computation\n",
    "3. ‚ö° Operate with extremely low power consumption\n",
    "\n",
    "### ü§î Why trade off power and accuracy?\n",
    "\n",
    "Traditional deep learning models running on GPUs or CPUs consume significant power (often hundreds of watts). In contrast, the human brain processes complex information while consuming only ~20 watts. Neuromorphic chips aim to bridge this efficiency gap by:\n",
    "\n",
    "- üìä Using spike-based computation\n",
    "- üéØ Implementing local learning rules\n",
    "- ‚ö° Exploiting sparse, event-driven processing\n",
    "\n",
    "However, these benefits often come with reduced accuracy compared to traditional deep learning approaches. Understanding and optimizing this trade-off is crucial for deploying neural networks in power-constrained environments like mobile devices or IoT sensors.\n",
    "\n",
    "## üìù Exercise overview\n",
    "\n",
    "In this exercise, you will:\n",
    "1. üîß Implement a simple neuromorphic chip simulator\n",
    "2. üèÉ‚Äç‚ôÇÔ∏è Train SNNs with different architectures\n",
    "3. üìä Analyze the power-accuracy trade-off\n",
    "4. üîç Explore how different parameters affect this trade-off\n",
    "\n",
    "**This will also serve as a solid introduction on how to effectively train SNNs using modern packages such as SNNTorch!**\n",
    "\n",
    "## üíª Setup\n",
    "\n",
    "Some of the code for this exercise is already provided, but you will need to implement some parts: \n",
    "\n",
    "### SNNModel (models.py)\n",
    "The `SNNModel` class implements a 2-layer Leaky Integrate-and-Fire (LIF) network using SNNTorch. The network architecture consists of:\n",
    "- Input layer ‚Üí Hidden layer (with LIF neurons) ‚Üí Output layer (with LIF neurons). (You will be able to play with other network architectures)\n",
    "- Each LIF neuron has a decay rate (beta) that controls how quickly the membrane potential decays. (You will be able to play with other neuron models provided by SNNTorch)\n",
    "- The network processes input data over multiple timesteps, producing spikes at each layer\n",
    "\n",
    "### NeuromorphicChip (chip.py)\n",
    "The `NeuromorphicChip` class simulates a neuromorphic hardware platform with the following constraints:\n",
    "- Maximum number of neurons: 1024\n",
    "- Maximum number of synapses: 64 * 1024\n",
    "- Memory per neuron: 32 bytes\n",
    "- Memory per synapse: 4 bytes\n",
    "- Energy consumption:\n",
    "  - 1e-1 nJ per neuron update\n",
    "  - 5e-4 nJ per synapse event\n",
    "  \n",
    "This backend hardware is very simple and does not include many features of neuromorphic hardware, and serves only as an introduction to thinking about efficient network design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install snntorch\n",
    "    !git clone https://github.com/neuro4ml/exercises.git\n",
    "    !cp exercises/w8-neuromorphic/*.py .\n",
    "    !cp exercises/w8-neuromorphic/dataset .\n",
    "    !cp exercises/w8-neuromorphic/dataset_labels .\n",
    "\n",
    "# If you are using a local machine, please install the dependencies yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For automatic reloading of external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from chip import NeuromorphicChip\n",
    "from models import SNNModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Exercise 1.1: Mapping Implementation\n",
    "\n",
    "To complete this first question you need to implement the functions necessary to map your network on the chip.\n",
    "\n",
    "- üìç Go to [models.py](models.py) and implement the `n_neurons` and `n_synapses` properties.\n",
    "- üìç Go to [chip.py](chip.py) and implement the `calculate_memory_usage`, `map` and `run` methods.\n",
    "- ‚ñ∂Ô∏è Run the following cell to check your implementation\n",
    "\n",
    "This is what you should see:\n",
    "\n",
    "    Simulation Results:\n",
    "    Energy consumption: 1.29 ¬µJ\n",
    "    Memory usage: 57.34 KB\n",
    "    Total neuron updates: 11000\n",
    "    Total synapse events: 389740\n",
    "    Average spike rate: 0.205\n",
    "    Total spikes: 3070.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip = NeuromorphicChip()\n",
    "\n",
    "dims = (128, 100, 10)\n",
    "n_timesteps = 100\n",
    "seed = 42\n",
    "snn = SNNModel(n_in=dims[0], n_hidden=dims[1], n_out=dims[-1], beta=0.95, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulation Results:\n",
      "Energy consumption: 0.29 ¬µJ\n",
      "Memory usage: 57.34 KB\n",
      "Total neuron updates: 110\n",
      "Total synapse events: 553716\n",
      "Average spike rate: 0.219\n",
      "Total spikes: 4327.0\n"
     ]
    }
   ],
   "source": [
    "# Generate random input (seed is fixed to 42 for reproducibility)\n",
    "torch.manual_seed(seed)\n",
    "input_data = torch.randn(n_timesteps, dims[0]) * 10  # 100 timesteps\n",
    "\n",
    "# Map the network on the chip\n",
    "chip.map(snn)\n",
    "# Run the network\n",
    "output, results = chip.run(input_data=input_data)\n",
    "\n",
    "print(\"\\nSimulation Results:\")\n",
    "print(f\"Energy consumption: {results['total_energy_nJ']/1000:.2f} ¬µJ\")\n",
    "print(f\"Memory usage: {results['memory_usage_bytes']/1024:.2f} KB\")\n",
    "print(f\"Total neuron updates: {results['neuron_updates']}\")\n",
    "print(f\"Total synapse events: {results['synapse_events']}\")\n",
    "print(f\"Average spike rate: {results['spike_rate']:.3f}\")\n",
    "print(f\"Total spikes: {results['total_spikes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö´ Exercise 1.2: Failed Mappings\n",
    "\n",
    "Now let's explore what happens when we try to map networks that exceed the chip's constraints:\n",
    "\n",
    "### üî¨ Experiments:\n",
    "1. üß† First, we'll try mapping a network with too many neurons\n",
    "2. üîó Then, we'll attempt to map one with too many synapses \n",
    "3. üí° Finally, we'll see how sparse connectivity can help fit larger networks\n",
    "\n",
    "Let's run these experiments and observe the error messages we get! Each case will demonstrate different limitations of neuromorphic hardware:\n",
    "The first two cases should return a `MemoryError` if your code is correct. The third case should run without errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many neurons: 1034 (max: 1024)\n"
     ]
    }
   ],
   "source": [
    "chip = NeuromorphicChip()\n",
    "\n",
    "# Case 1 : Too many neurons\n",
    "dims = (128, 1024, 10)\n",
    "seed = 42\n",
    "snn = SNNModel(n_in=dims[0], n_hidden=dims[1], n_out=dims[-1], beta=0.95, seed=seed)\n",
    "# Map the network on the chip\n",
    "try:\n",
    "    chip.map(snn)\n",
    "except MemoryError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many synapses: 70656 (max: 65536)\n"
     ]
    }
   ],
   "source": [
    "chip = NeuromorphicChip()\n",
    "\n",
    "# Case 2 : Too many synapses\n",
    "dims = (128, 512, 10)\n",
    "seed = 42\n",
    "snn = SNNModel(n_in=dims[0], n_hidden=dims[1], n_out=dims[-1], beta=0.95, seed=seed)\n",
    "# Map the network on the chip\n",
    "try:\n",
    "    chip.map(snn)\n",
    "except MemoryError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped! Memory usage: 154.16 KB, Number of neurons: 522, Number of synapses: 35289\n"
     ]
    }
   ],
   "source": [
    "# Case 3 : Sparse connectivity\n",
    "dims = (128, 512, 10)\n",
    "seed = 42\n",
    "snn = SNNModel(n_in=dims[0], n_hidden=dims[1], n_out=dims[-1], beta=0.95, seed=seed)\n",
    "for l in snn.layers:\n",
    "    if hasattr(l, \"weight\"):\n",
    "        l.weight.data = (\n",
    "            torch.rand(l.weight.data.shape) < 0.5\n",
    "        )  # 50% of the weights are non-zero\n",
    "\n",
    "# Map the network on the chip\n",
    "try:\n",
    "    chip.map(snn)\n",
    "    print(\n",
    "        f\"Mapped! Memory usage: {chip.calculate_memory_usage(snn)/1024:.2f} KB, Number of neurons: {snn.n_neurons}, Number of synapses: {snn.n_synapses}\"\n",
    "    )\n",
    "except MemoryError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 2: Training\n",
    "\n",
    "In this exercise you will train a SNN on the [Randman dataset](https://github.com/fzenke/randman).\n",
    "\n",
    "### üìä Background: The Randman Dataset\n",
    "\n",
    "The Randman dataset is a synthetic dataset specifically designed for training Spiking Neural Networks (SNNs). Here's what you need to know:\n",
    "\n",
    "1. **Dataset Structure**\n",
    "   - Generates labeled spike trains for classification\n",
    "   - Each sample consists of temporal spike patterns\n",
    "   - Data is organized into multiple classes (10 classes)\n",
    "   - Spike times are stored in `dataset` file\n",
    "   - Class labels are stored in `dataset_labels` file\n",
    "\n",
    "2. **Data Format**\n",
    "   - Input: Spike trains encoded as binary tensors (time x neurons)\n",
    "   - Each neuron can spike at different time steps\n",
    "   - Data is converted to one-hot encoding across time steps\n",
    "   - Shape: (batch_size, timesteps, input_neurons)\n",
    "\n",
    "3. **Classification Task**\n",
    "   - Goal: Classify input spike patterns into correct classes\n",
    "   - Output layer produces spike trains\n",
    "   - Classification is done using rate coding (for now !): the output neuron that spikes the most indicates the predicted class\n",
    "\n",
    "4. **Data Loading**\n",
    "   All necessary code for loading and preprocessing the data is provided:\n",
    "   - Data loading from files\n",
    "   - Conversion to one-hot encoding\n",
    "   - Train/test splitting\n",
    "   - DataLoader creation with batching\n",
    "\n",
    "### üéì 2.1 Training\n",
    "\n",
    "- üìù Go to [training.py](training.py) and complete the `SNNTrainer` class, in particular the `calculate_accuracy` method\n",
    "- ‚ñ∂Ô∏è Run the following cell to train your network\n",
    "- üìä Take a look at the training and testing metrics, especially the accuracy and energy consumption\n",
    "- üîÑ Start experimenting with different architectures and parameters to see how they affect the accuracy and energy consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import get_dataloaders, SNNTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100, 128]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "train_loader, test_loader, dataset = get_dataloaders(\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100, 128]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the data\n",
    "data, labels = next(iter(train_loader))\n",
    "print(\n",
    "    data.shape, labels.shape\n",
    ")  # batch_size x timesteps x n_in. 1st and 2nd dims are swapped when passed to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn_config = {\n",
    "    \"n_hidden\": 128,\n",
    "    \"beta\": 0.95,\n",
    "    \"seed\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "snn = SNNModel(\n",
    "    n_hidden=snn_config[\"n_hidden\"],\n",
    "    beta=snn_config[\"beta\"],\n",
    "    seed=snn_config[\"seed\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = SNNTrainer(snn, learning_rate=1e-3, lr_gamma=0.9, config=snn_config)\n",
    "# Train the model\n",
    "trainer.train(train_loader, test_loader, n_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà 2.2 Plot the results\n",
    "- üìä We can plot the accuracy and energy consumption as a function of the epoch\n",
    "- üìà We see that the accuracy is improving but the energy consumption is also increasing\n",
    "- ‚öñÔ∏è This is a trade-off that we need to be aware of when training SNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.pd_results.groupby(\"epoch\", as_index=False).mean()\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(\n",
    "    data=results, x=\"epoch\", y=\"accuracy\", ax=ax, label=\"Accuracy\", legend=False\n",
    ")\n",
    "ax2 = ax.twinx()\n",
    "sns.lineplot(\n",
    "    data=results,\n",
    "    x=\"epoch\",\n",
    "    y=\"total_energy_nJ\",\n",
    "    ax=ax2,\n",
    "    color=\"orange\",\n",
    "    label=\"Energy\",\n",
    "    legend=False,\n",
    ")\n",
    "ax.figure.legend()\n",
    "ax.set_title(\n",
    "    f\"Accuracy and Energy, Final Trade-off Score: {trainer.pareto_tradeoff:.2f}\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Exercise 3: Optimizing the trade-off\n",
    "\n",
    "Now, you will explore how different parameters affect the accuracy and energy consumption of the SNN. This part is open-ended, here are some ideas:\n",
    "\n",
    "-  Experiment with network architectures (number of layers, number of neurons, etc.)\n",
    "-  Regularize spiking activity \n",
    "-  Implement a bi-exponential neuron model, using SnnTorch (snn.neurons.Synaptic)\n",
    "- Implement a temporal loss (time-to-first-spike), using SnnTorch. Be careful to change the `calculate_accuracy` method in `training.py`\n",
    "-  Implement weight masks to reduce the number of synapses\n",
    "-  Use SnnTorch to make the time-constants heterogeneous and/or learnable, and maybe use less neurons\n",
    "\n",
    "Ideally, after experimenting with these parameters, you should start to see a rough trade-off between accuracy and energy! Can we see some kind of Pareto front appearing? \n",
    "\n",
    "### üèÜ *The group with the best trade-off score will win the competition!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro4ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
